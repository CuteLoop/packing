\chapter{Inquiry-Based Implementation of Stochastic Gradient Descent with Adam}

\section*{Guiding Question}
How can noisy, partial gradient information be systematically transformed into a stable, scalable optimization method for high-dimensional, nonconvex problems?

This chapter develops stochastic gradient descent (SGD) and the Adam optimizer through an inquiry-based sequence. The emphasis is on understanding the mathematical role of noise, momentum, and adaptivity, and on implementing Adam correctly from first principles.

\section{From Deterministic to Stochastic Gradients}

Consider the finite-sum optimization problem
\[
\min_{\theta \in \mathbb{R}^d} F(\theta) := \frac{1}{N} \sum_{i=1}^N f_i(\theta).
\]
\subsection{Inquiry}
\begin{itemize}
\item What happens if $N$ is very large?
\item Is it necessary to evaluate all $f_i$ at every iteration?
\item What if we replace the full gradient with an estimator?
\end{itemize}

This leads to stochastic gradients. Given a random index $i_k$, define
\[
g_k := \nabla f_{i_k}(\theta_k).
\]
Then $\mathbb{E}[g_k] = \nabla F(\theta_k)$, but $g_k$ has nonzero variance.

\section{Stochastic Gradient Descent}

The SGD iteration is
\[
\theta_{k+1} = \theta_k - \eta_k g_k.
\]
\subsection{Inquiry}
\begin{itemize}
\item Why does SGD converge despite noisy gradients?
\item What role does the learning rate $\eta_k$ play?
\item Why must $\eta_k \to 0$ in theory but not always in practice?
\end{itemize}

\subsection{Key Phenomena}
\begin{itemize}
\item Gradient noise acts as implicit regularization.
\item SGD escapes shallow local minima and saddle points.
\item Variance limits the achievable accuracy.
\end{itemize}

\section{Momentum as Time Averaging}

To reduce variance and accelerate convergence, introduce momentum:
\begin{align*}
v_{k+1} &= \beta v_k + (1-\beta) g_k, \\
\theta_{k+1} &= \theta_k - \eta v_{k+1}.
\end{align*}

\subsection{Inquiry}
\begin{itemize}
\item Why does averaging gradients reduce noise?
\item How is momentum related to low-pass filtering?
\item Why can momentum overshoot minima?
\end{itemize}

Momentum can be interpreted as discretizing a second-order differential equation with damping.

\section{Adaptive Learning Rates}

Different coordinates may have gradients of very different scales. Adaptive methods address this by normalizing updates.

Define the second-moment accumulator
\[
s_{k+1} = \beta_2 s_k + (1-\beta_2) g_k^2,
\]
where the square is taken elementwise.

\subsection{Inquiry}
\begin{itemize}
\item Why does dividing by $\sqrt{s_k}$ stabilize training?
\item What happens when gradients are sparse?
\item Why is per-coordinate adaptivity dangerous?
\end{itemize}

\section{The Adam Optimizer}

Adam combines momentum and adaptive scaling.

\subsection{Algorithm}
Initialize $m_0 = 0$, $v_0 = 0$.
\begin{align*}
m_{k+1} &= \beta_1 m_k + (1-\beta_1) g_k, \\
v_{k+1} &= \beta_2 v_k + (1-\beta_2) g_k^2.
\end{align*}

Bias correction:
\[
\hat m_{k+1} = \frac{m_{k+1}}{1-\beta_1^{k+1}},
\qquad
\hat v_{k+1} = \frac{v_{k+1}}{1-\beta_2^{k+1}}.
\]
Update:
\[
\theta_{k+1} = \theta_k - \eta \frac{\hat m_{k+1}}{\sqrt{\hat v_{k+1}} + \varepsilon}.
\]
\subsection{Inquiry}
\begin{itemize}
\item Why is bias correction necessary?
\item What goes wrong without $\varepsilon$?
\item Why does Adam often converge faster but generalize worse?
\end{itemize}

\section{Test-Driven Implementation Strategy}

We implement Adam using TDD to avoid silent bugs.

\subsection{Core Invariants to Test}
\begin{itemize}
\item Determinism given a fixed random seed.
\item Shape consistency of all tensors.
\item Non-negativity of second-moment estimates.
\item Correct bias correction at small $k$.
\end{itemize}

\section{Exercise Sequence (TDD)}

\subsection{Exercise 1: Gradient Oracle}
Write a test that checks a stochastic gradient estimator is unbiased on a quadratic function.

\subsection{Exercise 2: Momentum Averaging}
Verify that $m_k$ equals an exponential moving average of past gradients.

\subsection{Exercise 3: Second-Moment Accumulator}
Test that $v_k$ tracks the empirical variance scale of gradients.

\subsection{Exercise 4: Bias Correction}
On a constant gradient, verify that $\hat m_k$ converges immediately to the true gradient.

\subsection{Exercise 5: Adam Step on Quadratic Bowl}
Check that Adam converges to the minimizer of $f(\theta)=|\theta|^2$ from random initialization.

\section{Failure Modes}

\begin{center}
\begin{tabular}{ll}
\textbf{Symptom} & \textbf{Cause} \\
\hline
Divergence & Learning rate too large \\
Slow convergence & $\beta_1$ too small \\
Parameter drift & $\beta_2$ too large \\
Poor generalization & Excessive adaptivity
\end{tabular}
\end{center}

\section{Conceptual Summary}

Stochastic gradient methods trade deterministic descent for scalable, noise-tolerant optimization. Adam succeeds by combining three ideas: stochasticity for exploration, momentum for acceleration, and adaptive scaling for numerical stability. Understanding these components independently is essential for using Adam responsibly rather than as a black box.
